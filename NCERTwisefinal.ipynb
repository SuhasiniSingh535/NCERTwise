{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SuhasiniSingh535/NCERTwise/blob/main/NCERTwisefinal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "FALCON 7B INSTRUCT"
      ],
      "metadata": {
        "id": "3EKg0u6T1jsR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LCo038vxIzkC"
      },
      "outputs": [],
      "source": [
        "# ðŸš€ Install core libraries\n",
        "!pip install --upgrade torch torchvision --extra-index-url https://download.pytorch.org/whl/cu118\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7-oM2QZ0uzh"
      },
      "source": [
        "Installs all tools: PDF extraction, embeddings, vector search, LLM + 4-bit quantized training, dataset management.*italicised text*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jFrBjRspVjwL"
      },
      "outputs": [],
      "source": [
        "!pip install \"numpy<2\" transformers sentence-transformers peft faiss-cpu bitsandbytes triton==3.1.0\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JrrekrFEbDjN"
      },
      "outputs": [],
      "source": [
        "!pip install \"huggingface-hub>=0.30.0,<1.0\"\n",
        "\n",
        "# ðŸ§° Other tools\n",
        "!pip install pymupdf fsspec langchain accelerate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9085A7FO26Pl"
      },
      "outputs": [],
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!mkdir -p data/raw data/text\n",
        "!cp /content/drive/MyDrive/NCERT_PDFs/*.pdf data/raw/\n",
        "!ls data/raw/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3fHzkGQUZapS"
      },
      "outputs": [],
      "source": [
        "!pip uninstall -y torch torchvision torchaudio\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bialdy_pZca5"
      },
      "outputs": [],
      "source": [
        "!pip install torch torchvision --extra-index-url https://download.pytorch.org/whl/cu118\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tIXiRWyXZgST"
      },
      "outputs": [],
      "source": [
        "import torch, torchvision\n",
        "print(\"torch version:\", torch.__version__)\n",
        "print(\"torchvision version:\", torchvision.__version__)\n",
        "from torchvision.ops import nms\n",
        "print(\"âœ… torch & torchvision loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o9RsgzW8ZjiS"
      },
      "outputs": [],
      "source": [
        "# cell 3\n",
        "import os, pickle\n",
        "import pymupdf as fitz\n",
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Extract text\n",
        "os.makedirs(\"data/text\", exist_ok=True)\n",
        "texts = []\n",
        "for pdf in os.listdir(\"data/raw\"):\n",
        "    if pdf.endswith(\".pdf\"):\n",
        "        doc = fitz.open(f\"data/raw/{pdf}\")\n",
        "        text = \"\\n\".join([page.get_text() for page in doc])\n",
        "        texts.append({\"source\": pdf, \"text\": text})\n",
        "        open(f\"data/text/{pdf[:-4]}.txt\", \"w\", encoding=\"utf-8\").write(text)\n",
        "\n",
        "# Create index\n",
        "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "chunks = []\n",
        "for fname in os.listdir(\"data/text\"):\n",
        "    with open(f\"data/text/{fname}\", \"r\", encoding=\"utf-8\") as f:\n",
        "        txt = f.read()\n",
        "    for i in range(0, len(txt), 512):\n",
        "        chunks.append({\"text\": txt[i:i+512]})\n",
        "\n",
        "vecs = embedder.encode([c[\"text\"] for c in chunks], show_progress_bar=True)\n",
        "index = faiss.IndexFlatL2(len(vecs[0]))\n",
        "index.add(vecs)\n",
        "os.makedirs(\"data/index\", exist_ok=True)\n",
        "faiss.write_index(index, \"data/index/faiss.idx\")\n",
        "pickle.dump(chunks, open(\"data/index/chunks.pkl\", \"wb\"))\n",
        "print(f\"âœ… Indexed {len(chunks)} text chunks\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0nnQtHoyZp2K"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from datasets import Dataset, DatasetDict\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"tiiuae/falcon-7b-instruct\", trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "examples = []\n",
        "with open(\"ncertpdfs (1).jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        examples.append(json.loads(line))\n",
        "print(f\"âœ… Loaded {len(examples)} examples\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NM4s7LD8ZtZy"
      },
      "outputs": [],
      "source": [
        "# cell a\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "# 4-bit quantization config\n",
        "bnb_cfg = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16\n",
        ")\n",
        "\n",
        "# Load the quantized base model\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"alokabhishek/falcon-7b-instruct-bnb-4bit\",\n",
        "    quantization_config=bnb_cfg,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# Adjust vocabulary if needed\n",
        "base_model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# Set up LoRA\n",
        "lora_cfg = LoraConfig(\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    r=8, lora_alpha=8, lora_dropout=0.1,\n",
        "    target_modules=[\"query_key_value\", \"dense\"],\n",
        "    bias=\"none\"\n",
        ")\n",
        "model = get_peft_model(base_model, lora_cfg)\n",
        "model.eval()\n",
        "print(\"âœ… Model setup complete (quantized + LoRA)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CIZFMQsPZxR9"
      },
      "outputs": [],
      "source": [
        "# cell b\n",
        "from datasets import Dataset\n",
        "from transformers import DataCollatorForLanguageModeling, TrainingArguments, Trainer\n",
        "import torch\n",
        "\n",
        "ds = Dataset.from_list(examples)\n",
        "split = ds.train_test_split(test_size=0.1, seed=42)\n",
        "train_raw = split[\"train\"]\n",
        "val_raw = split[\"test\"]\n",
        "\n",
        "def tok_fn(ex):\n",
        "    prompts = [f\"Instruction: {i}\\nResponse: {o}\" for i, o in zip(ex[\"prompt\"], ex[\"completion\"])]\n",
        "    tok = tokenizer(prompts, truncation=True, padding=\"max_length\", max_length=256)\n",
        "    tok[\"labels\"] = [ids.copy() for ids in tok[\"input_ids\"]]\n",
        "    return tok\n",
        "\n",
        "train_ds = train_raw.map(tok_fn, batched=True, remove_columns=[\"prompt\", \"completion\"])\n",
        "val_ds = val_raw.map(tok_fn, batched=True, remove_columns=[\"prompt\", \"completion\"])\n",
        "\n",
        "collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"quant_falcon_lora\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=2,\n",
        "    learning_rate=5e-5,\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=100,\n",
        "    save_total_limit=3,\n",
        "    logging_dir=\"logs\",\n",
        "    report_to=\"wandb\",\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=100,\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model, args=training_args,\n",
        "    train_dataset=train_ds, eval_dataset=val_ds,\n",
        "    data_collator=collator, tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "print(\"âœ… Training setup complete â€” now training on your full dataset.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "wRCPP_A1Z0mP"
      },
      "outputs": [],
      "source": [
        "trainer.train()\n",
        "trainer.save_model(\"quant_falcon_lora_new_30_june\")\n",
        "tokenizer.save_pretrained(\"quant_falcon_lora_new_30_june\")\n",
        "print(\"âœ… Fine-tuning complete and saved to 'quant_falcon_lora_new_30_june'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Xx55Mf70Z3c7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd, math, matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "\n",
        "history = pd.DataFrame(trainer.state.log_history)\n",
        "train_hist = history[history.loss.notna()]\n",
        "eval_hist = history[history.eval_loss.notna()]\n",
        "\n",
        "plt.plot(train_hist.step, train_hist.loss.apply(math.exp), label=\"Train\")\n",
        "plt.plot(eval_hist.step, eval_hist.eval_loss.apply(math.exp), label=\"Val\")\n",
        "plt.legend(); plt.xlabel(\"Step\"); plt.ylabel(\"Perplexity\"); plt.grid()\n",
        "plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "qE2rcEdHZ-1m"
      },
      "outputs": [],
      "source": [
        "drive.mount('/content/drive', force_remount=True)\n",
        "trainer.save_model(\"/content/drive/MyDrive/quant_falcon_lora_final\")\n",
        "tokenizer.save_pretrained(\"/content/drive/MyDrive/quant_falcon_lora_final\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "bgvamP6naAm4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from peft import PeftModel, PeftConfig\n",
        "\n",
        "peft_path = \"quant_falcon_lora_new_30_june\"\n",
        "pc = PeftConfig.from_pretrained(peft_path)\n",
        "\n",
        "bnb_cfg = BitsAndBytesConfig(\n",
        "    load_in_4bit=True, bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True, bnb_4bit_compute_dtype=torch.float16,\n",
        "    llm_int8_enable_fp32_cpu_offload=True\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    pc.base_model_name_or_path, quantization_config=bnb_cfg, device_map=\"auto\", trust_remote_code=True\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(pc.base_model_name_or_path)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = PeftModel.from_pretrained(model, peft_path)\n",
        "model.eval()\n",
        "print(\"âœ… Quantized Falcon-LoRA loaded with offload\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9gxQde6daDeh"
      },
      "outputs": [],
      "source": [
        "import pickle, faiss\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "index = faiss.read_index(\"data/index/faiss.idx\")\n",
        "chunks = pickle.load(open(\"data/index/chunks.pkl\", \"rb\"))\n",
        "\n",
        "def retrieve_context(query, top_k=5):\n",
        "    q_emb = embedder.encode([query])\n",
        "    _, I = index.search(q_emb, top_k)\n",
        "    return [chunks[i][\"text\"] for i in I[0]]\n",
        "\n",
        "def rag_generate(query, top_k=3):\n",
        "    ctxs = retrieve_context(query, top_k)\n",
        "    prompt = \"Context:\\n\" + \"\\n\\n\".join(ctxs) + f\"\\n\\nQuestion: {query}\\nAnswer:\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(model.device)\n",
        "\n",
        "    raw = model.generate(\n",
        "        **inputs,\n",
        "        return_dict_in_generate=True,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        max_new_tokens=150,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        repetition_penalty=1.2,\n",
        "        no_repeat_ngram_size=3,\n",
        "        num_beams=1\n",
        "    )\n",
        "    seq = raw.sequences[0][inputs[\"input_ids\"].shape[1]:]\n",
        "    return tokenizer.decode(seq, skip_special_tokens=True)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "6JkQZ3soaGZK",
        "outputId": "84ec86f1-8b92-4abe-e7ce-c7bea011eebe"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'rag_generate' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1991855317.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrag_generate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"what do we mean by democracy?answer in approx 160 words \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'rag_generate' is not defined"
          ]
        }
      ],
      "source": [
        "print(rag_generate(\"what do we mean by democracy?answer in approx 160 words \"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAr5JX3TxFSA"
      },
      "source": [
        "for using the trained and saved model"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}